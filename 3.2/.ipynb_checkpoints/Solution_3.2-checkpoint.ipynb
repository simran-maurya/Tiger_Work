{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e04b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflowNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Using cached protobuf-4.21.1-cp39-cp39-win_amd64.whl (524 kB)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.7.tar.gz (67 kB)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (8.0.3)\n",
      "Collecting waitress\n",
      "  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (1.3.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (2.0.0)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (1.4.22)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (6.0)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (21.0)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Using cached prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (1.7.1)\n",
      "Requirement already satisfied: Flask in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (4.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (1.20.3)\n",
      "Collecting alembic\n",
      "  Using cached alembic-1.8.0-py3-none-any.whl (209 kB)\n",
      "Requirement already satisfied: requests>=2.17.3 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (2.26.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from mlflow) (2021.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from click>=7.0->mlflow) (0.4.4)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (2.1.0)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (1.16.0)\n",
      "Collecting pywin32==227\n",
      "  Using cached pywin32-227-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from docker>=4.0.0->mlflow) (1.3.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\simran.maurya\\\\Anaconda3\\\\Lib\\\\site-packages\\\\pywin32_system32\\\\pywintypes39.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (3.2)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from sqlalchemy->mlflow) (1.1.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from Flask->mlflow) (2.0.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from Flask->mlflow) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from Flask->mlflow) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from packaging->mlflow) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from pandas->mlflow) (2.8.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.7-py3-none-any.whl size=119057 sha256=3521580baa1445d27f339acfdc3c2d88a8be84b5e2b800767551133f8a6833a2\n",
      "  Stored in directory: c:\\users\\simran.maurya\\appdata\\local\\pip\\cache\\wheels\\6d\\f2\\45\\71d2e8b007fc992cc4f38af3982a728a53fc830290a1c2c68c\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: pywin32, oauthlib, Mako, gitdb, waitress, sqlparse, querystring-parser, protobuf, prometheus-flask-exporter, gitpython, docker, databricks-cli, alembic, mlflow\n",
      "  Attempting uninstall: pywin32\n",
      "    Found existing installation: pywin32 228\n",
      "    Uninstalling pywin32-228:\n",
      "      Successfully uninstalled pywin32-228\n",
      "  Rolling back uninstall of pywin32\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\__pycache__\\pythoncom.cpython-39.pyc\n",
      "   from C:\\Users\\simran.maurya\\AppData\\Local\\Temp\\pip-uninstall-mesnsxeh\\pythoncom.cpython-39.pyc\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\adodbapi\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~dodbapi\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\isapi\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~sapi\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\pythoncom.py\n",
      "   from C:\\Users\\simran.maurya\\AppData\\Local\\Temp\\pip-uninstall-p82yfjxv\\pythoncom.py\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\pythonwin\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~ythonwin\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\pywin32-228-py3.9.egg-info\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~ywin32-228-py3.9.egg-info\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\win32\\lib\\\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\win32\\~ib\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\win32com\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~in32com\n",
      "  Moving to c:\\users\\simran.maurya\\anaconda3\\lib\\site-packages\\win32comext\n",
      "   from C:\\Users\\simran.maurya\\Anaconda3\\Lib\\site-packages\\~in32comext\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd624b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SIMRAN~1.MAU\\AppData\\Local\\Temp/ipykernel_30928/3657679153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b732202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from six.moves import urllib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c695acea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:5000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_server_uri = \"http://127.0.0.1:5000\" # set to your server URI\n",
    "mlflow.set_tracking_uri(remote_server_uri)  # or set the MLFLOW_TRACKING_URI in the env\n",
    "mlflow.tracking.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b019d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlruns/1', experiment_id='1', lifecycle_stage='active', name='Housing_Price_Prediction_Experiment', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"Housing_Price_Prediction_Experiment\"\n",
    "mlflow.set_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd2c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Random Forest Model --\n",
      "\n",
      "  RMSE: 47682.77540712766\n",
      "  MAE: 31769.932299741602\n",
      "  R2: 0.8255489237395524\n",
      "Save to: mlruns/1/c87ff8978a834714a61bc93373594579/artifacts\n",
      "\n",
      "-- Linear Regression Model --\n",
      "\n",
      "  RMSE: 68627.87390018745\n",
      "  MAE: 49438.66860915801\n",
      "  R2: 0.6481553634454353\n",
      "Save to: mlruns/1/fdc8d28fa462438d97b825c24d1bac8f/artifacts\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n",
    "    mlflow.log_param(\"parent\", \"yes\")\n",
    "    with mlflow.start_run(run_name='MATRIX_EVALUATION', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "        def eval_metrics(actual, pred):\n",
    "            # compute relevant metrics\n",
    "            rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "            mae = mean_absolute_error(actual, pred)\n",
    "            r2 = r2_score(actual, pred)\n",
    "            return rmse, mae, r2\n",
    "        \n",
    "    \n",
    "    with mlflow.start_run(run_name='LOADING_DATA', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "        def load_data():\n",
    "    \n",
    "            DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "            HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "            HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "            def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "                if not os.path.isdir(housing_path):\n",
    "                    os.makedirs(housing_path)\n",
    "                tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "                urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "                housing_tgz = tarfile.open(tgz_path)\n",
    "                housing_tgz.extractall(path=housing_path)\n",
    "                housing_tgz.close()\n",
    "\n",
    "            def load_housing_data(housing_path=HOUSING_PATH):\n",
    "                csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "                return pd.read_csv(csv_path)\n",
    "            fetch_housing_data()\n",
    "            housing = load_housing_data()\n",
    "            return housing\n",
    "        \n",
    "    with mlflow.start_run(run_name='SAMPLING_DATA', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "        #categorizing median income to perform stratified sampling.\n",
    "        housing = load_data()\n",
    "        housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                                   bins=[0., 1.5, 3.0, 4.5, 6.,np.inf],\n",
    "                                   labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "        #Performing stratified sampling.\n",
    "\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "            strat_train_set = housing.loc[train_index]\n",
    "            strat_test_set = housing.loc[test_index]\n",
    "        for set_ in (strat_train_set, strat_test_set):\n",
    "            set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "        housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "        housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "    with mlflow.start_run(run_name='CLEANING_DATA', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "\n",
    "        # Data cleaning\n",
    "\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        housing_num = housing.drop(\"ocean_proximity\", axis=1) # Dropped Ocean_Proximity as it is a non-numeric column.\n",
    "        imputer.fit(housing_num)\n",
    "        X = imputer.transform(housing_num)\n",
    "        housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "        housing_cat = housing[[\"ocean_proximity\"]]\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        cat_encoder = OneHotEncoder()\n",
    "        housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # Creating Dummy clomns for non-numeric data\n",
    "\n",
    "    with mlflow.start_run(run_name='CUSTOM_TRANSFORMER', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "        from sklearn.base import BaseEstimator, TransformerMixin\n",
    "        rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "        class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "            def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "                self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "            def fit(self, X, y=None):\n",
    "                return self # nothing else to do\n",
    "            def transform(self, X, y=None):\n",
    "                rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "                population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "                if self.add_bedrooms_per_room:\n",
    "                    bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "                    return np.c_[X, rooms_per_household, population_per_household,\n",
    "                    bedrooms_per_room]\n",
    "                else:\n",
    "                    return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "        attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "        housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "    with mlflow.start_run(run_name='PIPELINE', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "\n",
    "        #Transformation Pipelines\n",
    "\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        num_pipeline = Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy=\"median\")), ## Imputing missing values\n",
    "         ('attribs_adder', CombinedAttributesAdder()),  ## combining attributes to make them logical\n",
    "         ('std_scaler', StandardScaler()),              ## Standardising features\n",
    "         ])\n",
    "        housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "        #Full pipeline for both categorical and numerical data columns \n",
    "\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        num_attribs = list(housing_num)\n",
    "        cat_attribs = [\"ocean_proximity\"]\n",
    "        full_pipeline = ColumnTransformer([\n",
    "         (\"num\", num_pipeline, num_attribs),\n",
    "         (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "         ])\n",
    "        housing_prepared = full_pipeline.fit_transform(housing)\n",
    "    with mlflow.start_run(run_name='TRAINING_MODEL', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", \"yes\")\n",
    "        # train a model with given parameters\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        np.random.seed(40) \n",
    "\n",
    "        # Making a Random Forest Model\n",
    "\n",
    "\n",
    "        with mlflow.start_run(run_name='RANDOM_FOREST_MODEL', nested=True) as child_run:\n",
    "            mlflow.log_param(\"child\", \"yes\")\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            forest_reg = RandomForestRegressor()\n",
    "            forest_reg.fit(housing_prepared, housing_labels)\n",
    "            housing_predictions = forest_reg.predict(housing_prepared)\n",
    "\n",
    "            #Performing Grid Search\n",
    "\n",
    "            from sklearn.model_selection import GridSearchCV\n",
    "            param_grid = [\n",
    "                {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "                {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "             ]\n",
    "            forest_reg = RandomForestRegressor()\n",
    "            grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                                       scoring='neg_mean_squared_error',\n",
    "            return_train_score=True)\n",
    "            grid_search.fit(housing_prepared, housing_labels)\n",
    "            cvres = grid_search.cv_results_\n",
    "            feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "            feature_importances\n",
    "\n",
    "            extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "            cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "            cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "            attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "            sorted(zip(feature_importances, attributes), reverse=True)\n",
    "\n",
    "            final_model = grid_search.best_estimator_\n",
    "            X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "            y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "            X_test_prepared = full_pipeline.transform(X_test)\n",
    "            final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "            # Evaluate Metrics\n",
    "\n",
    "            (rmse, mae, r2) = eval_metrics(y_test, final_predictions)\n",
    "\n",
    "            print(\"\\n-- Random Forest Model --\\n\")\n",
    "            print(\"  RMSE: %s\" % rmse)\n",
    "            print(\"  MAE: %s\" % mae)\n",
    "            print(\"  R2: %s\" % r2)\n",
    "\n",
    "            # metrics, and model to MLflow\n",
    "            mlflow.log_param(\"Model\", \"Random Forest Model\")\n",
    "            mlflow.log_metric(key=\"rmse\", value=rmse)\n",
    "            mlflow.log_metrics({\"mae\": mae, \"r2\": r2})\n",
    "            print(\"Save to: {}\".format(mlflow.get_artifact_uri()))\n",
    "\n",
    "            mlflow.sklearn.log_model(forest_reg, \"model\")\n",
    "\n",
    "        with mlflow.start_run(run_name='LINEAR_REGRESSION_MODEL', nested=True) as child_run:\n",
    "            mlflow.log_param(\"child\", \"yes\")\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            lin_reg = LinearRegression()\n",
    "            lin_reg.fit(housing_prepared, housing_labels)\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "            housing_predictions = lin_reg.predict(housing_prepared)\n",
    "\n",
    "            # Evaluate Metrics\n",
    "\n",
    "            (rmse, mae, r2) = eval_metrics(housing_labels, housing_predictions)\n",
    "\n",
    "            print(\"\\n-- Linear Regression Model --\\n\")\n",
    "            print(\"  RMSE: %s\" % rmse)\n",
    "            print(\"  MAE: %s\" % mae)\n",
    "            print(\"  R2: %s\" % r2)\n",
    "\n",
    "            # metrics, and model to MLflow\n",
    "            mlflow.log_param(\"Model\", \"Linear Regression Model\")\n",
    "            mlflow.log_metric(key=\"rmse\", value=rmse)\n",
    "            mlflow.log_metrics({\"mae\": mae, \"r2\": r2})\n",
    "            print(\"Save to: {}\".format(mlflow.get_artifact_uri()))\n",
    "\n",
    "            mlflow.sklearn.log_model(lin_reg, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd63e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
